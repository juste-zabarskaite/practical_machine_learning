---
title: 'Practical Machine Learning: Peer-graded Assignment: Prediction Assignment Writeup'
author: "Juste Zabarskaite"
date: "December 20, 2016"
output: html_document
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE, results='hide', error=FALSE}
packages <- c( 'knitr', 'caret', 'AppliedPredictiveModeling', 'ElemStatLearn', 'pgmm', 'rpart', 'gbm', 'lubridate', 'forecast', 'e1071', 'rattle', 'rpart.plot', 'randomForest', 'RANN', 'MASS', 'ranger', 'xgboost')
for (package in packages) {
    if (!require(package, character.only=T, quietly=T)) {
        install.packages(package)
        library(package, character.only=T)
    }
}
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The purpose of this project is to build a model to predict quality ('classe') of the exercise.

## Packages & Downloading Data

Install packages `r packages`.

````{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE, results='hide', error=FALSE}
# make this an external chunk that can be included in any file
options(width = 100)
library(knitr)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
runif(1)
```

Download [Weight Lifting Exercise Dataset](http://groupware.les.inf.puc-rio.br/har). 

```{r, cache = TRUE}
source_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
source_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
destfile_train = "./pml-training.csv"
destfile_test = "./pml-testing.csv"
## download files
if(!file.exists(destfile_train)) { download.file(source_train,destfile_train) }
if(!file.exists(destfile_test)) { download.file(source_test,destfile_test) }
## import data
training_orig <- read.csv(destfile_train, na.strings=c("NA","#DIV/0!","") )
validation_orig <- read.csv(destfile_test, na.strings=c("NA","#DIV/0!","") )
outcome <- "classe"
```

The training set contains `r dim(training_orig)[1]` observations with `r dim(training_orig)[2]` variables. The validation data set contains `r dim(validation_orig)[1]` observations with `r dim(validation_orig)[2]` variables. 

## Data Processing

Add new 'hour' to capture time series effect, if any.

```{r}
training <- training_orig
training$raw_timestamp_part_1_POSIXct <- as.POSIXct(as.numeric(as.character(training$raw_timestamp_part_1)),origin="1970-01-01",tz="GMT")
training$hours <- as.numeric(format(training$raw_timestamp_part_1_POSIXct,'%H'))
## ditto for validation data set
validation <- validation_orig
validation$raw_timestamp_part_1_POSIXct <- as.POSIXct(as.numeric(as.character(validation$raw_timestamp_part_1)),origin="1970-01-01",tz="GMT")
validation$hours <- as.numeric(format(validation$raw_timestamp_part_1_POSIXct,'%H'))
```

Exclude predictors that are missing (NA) in the validation data set (they will not help even if they have prediction power in the training set).

```{r}
validation <- validation[colSums(is.na(validation)) < 1*nrow(validation)]
## keep the same columns for the training set
cols <- names(validation)
training <- training[ , which(names(training) %in% c(cols,outcome))]
```

Exclude time stamp & index predictors that can mislead the model.

```{r}
colsrm <- c("X", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "raw_timestamp_part_1_POSIXct")
training <- training[ , -which(names(training) %in% c(colsrm))]
validation <- validation[ , -which(names(validation) %in% c(colsrm))]
```

The training set contains `r dim(training)[1]` observations with `r dim(training)[2]` variables.

Check predictors with >20% missing values.

```{r}
colnames(training)[colSums(is.na(training)) >= 0.2*nrow(training)]
```

No such predictors. No need to remove anything.

Exclude predictors with low variance.

```{r}
to_exclude <- nearZeroVar(training)
training2 <- training[, -to_exclude]
## ditto for validation set
validation2 <- validation[, -to_exclude]
```

The processed set contains `r dim(training2)[1]` observations with `r dim(training2)[2]` variables (reduction from original `r dim(training_orig)[2]` variables).

### Create Test Set

```{r}
set.seed(123)
partition <- createDataPartition(training$classe, p = 0.75, list = FALSE)
training3 <- training2[partition,]
testing3 <- training2[-partition,]
```

Training set now contains `r dim(training3)[1]` observations with `r dim(training3)[2]` variables.

Testing set now contains `r dim(testing3)[1]` observations with `r dim(testing3)[2]` variables.

### Fit alternative models

- Random Forest (rf)
- Stochastic Gradient Boosting (gbm)
- Conditional Inference Tree (ctree)
- Linear Discriminant Analysis (lda)
- CART (rpart)

```{r, echo=FALSE, cache=TRUE}
set.seed(62433)
train_control<- trainControl(method="cv", number=3, savePredictions = TRUE)

mod_rf <- train(classe ~ ., data=training3, method="rf", ntree=500, trControl=train_control, preProc=c("center", "scale", "BoxCox"))

mod_gbm <- train(classe ~ ., data=training3, method="gbm", trControl=train_control, verbose = FALSE)

mod_ctree <- train(classe ~ ., data=training3, method = "ctree", trControl=train_control, preProc = c("center", "scale", "BoxCox"))

mod_lda = train(classe ~ ., data = training3, method = 'lda', trControl=train_control, preProc = c("center", "scale", "BoxCox") )

mod_rpart <- train(classe ~ ., data=training3, method="rpart", trControl=train_control, preProc=c("center", "scale"))
```

Combining predictor models.

```{r}
t_rf <- predict(mod_rf, testing3)
t_gbm <- predict(mod_gbm, testing3)
t_ctree <- predict(mod_ctree, testing3)
t_lda <- predict(mod_lda, testing3)
t_rpart <- predict(mod_rpart, testing3)

predDF <- data.frame(t_rf,t_gbm,t_ctree,t_lda,t_rpart,classe=testing3$classe)
mod_comb <- train(classe ~.,method="gam",data=predDF)
t_comb <- predict(mod_comb,predDF)
```

### Check models on testing set

```{r}
cm_rf2 <- confusionMatrix(t_rf, testing3$classe)
cm_gbm2 <- confusionMatrix(t_gbm, testing3$classe)
cm_ctree2 <- confusionMatrix(t_ctree, testing3$classe)
cm_lda2 <- confusionMatrix(t_lda, testing3$classe)
cm_rpart2 <- confusionMatrix(t_rpart, testing3$classe)
cm_comb2 <- confusionMatrix(t_comb, testing3$classe)

df2 <- data.frame(method=c("mod_rf", "mod_gbm", "mod_ctree", "mod_lda", "mod_rpart", "mod_comb"), accuracy=c( cm_rf2$overall['Accuracy'], cm_gbm2$overall['Accuracy'], cm_ctree2$overall['Accuracy'], cm_lda2$overall['Accuracy'], cm_rpart2$overall['Accuracy'], cm_comb2$overall['Accuracy'] ))
df2 <- df2[with(df2, order(-accuracy)), ]
df2
```

The model with highest accuracy is `r df2$method[1]` with accuracy `r df2$accuracy[1]`.

## Predict values for validation set

```{r}
## predict
pred_rf <- predict(mod_rf, validation2)
pred_gbm <- predict(mod_gbm, validation2)
pred_ctree <- predict(mod_ctree, validation2)
pred_lda <- predict(mod_lda, validation2)
pred_rpart <- predict(mod_rpart, validation2)

df <- data.frame(pred_rf,pred_gbm,pred_ctree,pred_lda,pred_rpart )
df
```